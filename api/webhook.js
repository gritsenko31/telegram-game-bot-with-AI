const { Telegraf } = require('telegraf');
const { GoogleGenerativeAI } = require('@google/generative-ai');

const bot = new Telegraf(process.env.BOT_TOKEN);
const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);

// Ð¥Ñ€Ð°Ð½Ð¸Ð»Ð¸Ñ‰Ðµ Ð´Ð»Ñ ÑÐµÑÑÐ¸Ð¹
const userSessions = new Map();

const MESSAGE_CHUNK_SIZE = 4000;

// ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð¸Ð»Ð¸ ÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ ÑÐµÑÑÐ¸ÑŽ
function getUserSession(userId) {
  if (!userSessions.has(userId)) {
    const model = genAI.getGenerativeModel({ 
      model: 'gemini-2.5-flash',
      generationConfig: {
        temperature: 0.9,
        topK: 40,
        topP: 0.95,
        maxOutputTokens: 8192,
      }
    });
    
    const chat = model.startChat({ history: [] });
    
    userSessions.set(userId, {
      chat,
      messageCount: 0
    });
  }
  return userSessions.get(userId);
}

// Ð Ð°Ð·Ð±Ð¸Ð²ÐºÐ° Ð´Ð»Ð¸Ð½Ð½Ñ‹Ñ… ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹
function splitMessage(text, maxLength = MESSAGE_CHUNK_SIZE) {
  const chunks = [];
  let currentChunk = '';
  const lines = text.split('\n');
  
  for (const line of lines) {
    if ((currentChunk + line + '\n').length > maxLength) {
      if (currentChunk) chunks.push(currentChunk.trim());
      currentChunk = line + '\n';
    } else {
      currentChunk += line + '\n';
    }
  }
  
  if (currentChunk) chunks.push(currentChunk.trim());
  return chunks;
}

// ÐšÐ¾Ð¼Ð°Ð½Ð´Ð° /start
bot.start((ctx) => {
  const welcomeMessage = `ðŸ‘‹ Hello! I'm an AI bot powered by Gemini 2.5 Flash.

ðŸ“ I understand context and remember our conversation.

ðŸ”§ Commands:
/clear - Clear chat history
/help - Show help

Just send me a message!`;
  
  return ctx.reply(welcomeMessage);
});

// ÐšÐ¾Ð¼Ð°Ð½Ð´Ð° /help
bot.command('help', (ctx) => {
  const helpMessage = `â„¹ï¸ Bot Help:

/start - Start the bot
/clear - Clear conversation history
/help - Show this help

ðŸ’¡ Tip: I remember our conversation context!`;
  
  return ctx.reply(helpMessage);
});

// ÐšÐ¾Ð¼Ð°Ð½Ð´Ð° /clear
bot.command('clear', (ctx) => {
  const userId = ctx.from.id;
  userSessions.delete(userId);
  return ctx.reply('âœ… Chat history cleared!');
});

// ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ñ… ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹
bot.on('text', async (ctx) => {
  const userId = ctx.from.id;
  const userMessage = ctx.message.text;
  
  if (userMessage.startsWith('/')) return;
  
  try {
    await ctx.sendChatAction('typing');
    
    const session = getUserSession(userId);
    const result = await session.chat.sendMessage(userMessage);
    const aiResponse = result.response.text();
    
    session.messageCount++;
    
    const chunks = splitMessage(aiResponse);
    
    for (const chunk of chunks) {
      await ctx.reply(chunk);
      if (chunks.length > 1) {
        await new Promise(resolve => setTimeout(resolve, 500));
      }
    }
  } catch (error) {
    console.error('Error:', error);
    
    let errorMessage = 'âŒ An error occurred: ' + error.message;
    
    if (error.message.includes('429')) {
      errorMessage = 'âš ï¸ Rate limit exceeded. Try again in a minute.';
    } else if (error.message.includes('SAFETY')) {
      errorMessage = 'âš ï¸ Content filtered. Try rephrasing.';
    }
    
    await ctx.reply(errorMessage);
  }
});

// Vercel Serverless Function Handler
module.exports = async (req, res) => {
  try {
    if (req.method === 'POST') {
      await bot.handleUpdate(req.body);
      res.status(200).json({ ok: true });
    } else {
      res.status(200).json({ status: 'Bot is running on Vercel!' });
    }
  } catch (error) {
    console.error('Webhook error:', error);
    res.status(500).json({ error: error.message });
  }
};
